\documentclass[12pt]{article}
\usepackage{sbc-template}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{times,amsmath,epsfig}
\usepackage{graphicx,url}
  \makeatletter
  \newif\if@restonecol
  \makeatother
  \let\algorithm\relax
  \let\endalgorithm\relax
\usepackage[lined,algonl,ruled]{algorithm2e}
\usepackage{multirow}
\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{hyperref}

\sloppy

\title{Algoritmos e Estruturas de Dados 3 \\ Trabalho Prático 4 \\
\huge{CDFs! \\ Ordenação em memória externa}}
\date{November 22, 1963}


\author{André Taiar Marinho Oliveira}


\address{Departamento de Ciência da Computação -- Universidade Federal de Minas Gerais (UFMG)
\email{taiar@dcc.ufmg.br}
\\
\\ Novembro de 2010
}

\begin{document}

\maketitle

\begin{resumo}
Muitas aplicações em computação necessitam trabalhar com volumes de dados maiores do que
o que a memória principal (ou memória interna) das máquinas suporta. Tais aplicações precisam
fazer uso dos discos dos computadores que fornecem uma quantidade de espaço ordens de grandeza 
maior e suportam o volume de dados a ser computado, pagando para isso o preço da eficiência (dados 
em disco demoram ordens de grandeza de tempo a mais para serem acessados do que em memória interna).
Neste trabalho aplicaremos um método de ordenação em memória secundária para resolver o problema
das CDFs (\textit{Cumulative Distribution Function}) com um volume extenso de dados.
\end{resumo}

\section{Introdução}

A função de probabilidade acumulada (ou CDF) descreve completamente a
distribuição da probabilidade de uma variável aleatória de valor real $X$. 
Para cada número real $X$, a CDF é:

$F(x) = P(X =< x)$

Isso significa que, dado um número qualquer $X$ a CDF indica a probabilidade dos
outros valores $X$ serem menores ou iguais a $x$.

O algoritmo utilizado neste trabalho para calcular a CDF de um conjunto de
números em ponto flutuante é descrito no algoritmo \ref{alg:cdf}.

\begin{algorithm}[h!]
\label{alg:cdf}
\begin{footnotesize}
$D \longleftarrow $ conjunto muito grande de números (floats)\;
\tcp{Esta é a função mais importante deste trabalho.}
\textbf{ORDENA}$(D)$\; 
$count \longleftarrow$ 0\;
$current \longleftarrow$ $D[0]$\;
$len \longleftarrow$ TAMANHO$(D)$\;
\ForEach{$n \in D$}{
  \If{$n \neq current$}{
    $current \longleftarrow n$\;
    $prob \longleftarrow count/len$\;
    ESCREVE$(current, prob)$\;
  }
  $count \longleftarrow count + 1$\;
}
$n \longleftarrow D[len - 1]$\;
ESCREVE$(n, 1)$\;
\caption{Cálculo da CDF}
\end{footnotesize}
\end{algorithm}

O algoritmo de cálculo da CDF é bem simples, porém, como o volume de dados a ser
fornecido para ele é grande o suficiente para não caber na memória interna do
computador, isso faz com que o passo de ordenação dos dados se torne algo mais
complicado (necessita de ser ordenado na memória externa). Para cumprir essa parte
do trabalho, utilizaremos uma técnica chamada \textbf{Intercalação Balanceada de 
Vários Caminhos}.

\section{Solução Proposta}
\label{solucao_proposta}

O método de Intercalação Balanceada de Vários Caminhos, lê partes arbitrárias do 
aquivo de entrada a ser ordenado (partes essas que cabem em memória principal e 
podem ser manipuladas assim), as ordena e salva em arquivos temporários. Ao final 
do processo, quando todo o arquivo inicial está fragmentado em arquivos temporários
menores e ordenados, começa o processo de intercalação do algoritmo, onde os 
temporários são combinados na solução final obtendo-se o arquivo inicial ordenado.

Intercalar significa combinar dois ou mais blocos ordenados em um único bloco ordenado.
A intercalação, neste caso, é utilizada como uma operação auxiliar na ordenação. O 
processo completo utilizado para a ordenação neste trabalho é o visto no algoritmo 
\ref{alg:ord}.

\begin{algorithm}[h!]
\label{alg:ord}
\begin{footnotesize}

$file \longleftarrow$ aponta para o arquivo de entrada\;
$maxSize \longleftarrow$ máximo de valores a serem lidos\;
$v[maxSize] \longleftarrow$ vetor alocado com o tamanho de $maxSize$\;
$count \longleftarrow 0$\;
$tempId \longleftarrow 0$\;
\tcp{Gera temporários}
\While{houver dados em file}
{
	\While{$count < maxSize$}
	{
		$v[count] \longleftarrow$ READ$(file)$\;
		$count \longleftarrow count + 1$\;
	}
	ORDENA\_EM\_MEMORIA\_INTERNA($v$)\;
	ESCREVE\_TEMP($v, tempId$)\;
	$tempId \longleftarrow tempId + 1$\;
}
\tcp{Intercala}
$temps[tempId] \longleftarrow$ vetor alocado com o tamanho de $tempId$\;
\ForEach{arquivo temporário gerado $F_{i}$}
{
	$temps[i] \longleftarrow$ aponta para conteúdo do arquivo $F_{i}$\;
}
\While{houver dados em algum aquivo temporário $F_{i}$}
{
	$leitura \longleftarrow$ SELECIONA\_MENOR\_VALOR(temps);
	ESCREVE\_SAIDA($leitura$);
}

\caption{Intercalação Balanceada de Vários Caminhos}
\end{footnotesize}
\end{algorithm}



\section{Código}

\subsection{Módulos}
O programa foi separado em quatro módulos:
\begin{itemize}
\item \textbf{Grafo:} contém todas as operações executadas sobre a 
estrutura de dados Grafo, que foi utilizada para armazenamento e cálculo
das distâncias entre as cidades.
\item \textbf{Entrada e Saída:} controla a leitura do arquivo de distâncias
e a escrita do resultado do programa na saída padrão. Também faz a validação dos 
argumentos de entrada pela linha de comando.
\item \textbf{Geografia:} contém dados como nomes das cidades (para sua identificação
e armazenamento de seu identificador único), descrição das regiões do brasil
segundo as capitais de seus estados e algumas operações sobre esses dados geográficos.
\item \textbf{Principal:} interagiu com todos os outros módulos retornando erro
quando alguma irregularidade era colocada, controlou o fluxo de execução do
programa.
\end{itemize}

\subsection{Entrada e saída}
Para não haver problemas com a leitura da linha de comando, foi utilizado um utilitário do
sistema Linux chamado \textbf{getopt} que permite uma leitura mais robusta dos argumentos
passados ao programa. O comando para execução do programa definido para a linha de entrada 
foi:
\begin{verbatim}
#: ./tp3 -a <A> -b <B> -c <C> -d <D> -e <E> -f <F> -g <G> 
\end{verbatim}
Sendo: 
\begin{itemize}
  \item \textbf{A} - Quantidade total de dias de viagem;
  \item \textbf{B} - Cidade em que a viagem começa (e consequentemente aonde deve terminar);
  \item \textbf{C} - Dinheiro disponível para o gasto com esta viagem;
  \item \textbf{D} - Quantidade máxima de dias que um candidato pode ficar sem visitar alguma
  região geográfica do Brasil.
  \item \textbf{E} - Preço por Km para viajar de Carro;
  \item \textbf{F} - Preço por Km para viajar de Avião; 
  \item \textbf{G} - Algoritmo a ser exevutado: solução ótima (\textit{otima}) ou a 
  heurística (\textit{heuristica}).
\end{itemize}

% falar sobre o arquivo de distancias
O arquivo de distâncias especifica a distância em Km entre cada uma das capitais brasileiras
no seguinte formato:
\begin{verbatim}
...
<A> <B> <C> <D>
...
\end{verbatim}
Sendo:
\begin{itemize}
  \item \textbf{A} - nome da cidade A;
  \item \textbf{B} - nome da cidade B; 
  \item \textbf{C} - distância rodoviária entre a cidade A a cidade B;
  \item \textbf{D} - distância aérea entre a cidade A e a cidade B. 
\end{itemize}

% falar sobre o formato da saida
A saída do programa é retornada na saída padrão do sistema no seguinte formato:
\begin{verbatim}
...
<A> <B> <C>
...
\end{verbatim}
Sendo:
\begin{itemize}
  \item \textbf{A} - número do dia em que o candidato chegou à cidade;
  \item \textbf{B} - nome da cidade em que o candidato chegou; 
  \item \textbf{C} - meio de transporte utilizado pelo candidato para ir até a próxima cidade.
\end{itemize}

\subsection{Compilação}
O compilador utilizado neste trabalho foi o GCC (adotado como padrão para a disciplina) e 
o comando para compilar o programa através do GCC é:
\begin{verbatim}
  #: gcc -o tp3 tp3.c io.c io.h geografia.c geografia.h grafo.c grafo.h
\end{verbatim}
caso estejam todos os arquivos dentro do mesmo diretório.

Como pedido na especificação, foi feito um arquivo Makefile com o qual é possível compilar
o programa com o comando: 
\begin{verbatim}
  #: make all
\end{verbatim}

E podemos compilar e executar o programa com o comando:
\begin{verbatim}
  #: make run
\end{verbatim}
que fará, além da compilação, com que o programa execute com uma entrada arbitrária.

\section{Avaliação Experimental}
\label{avaliacao_experimental}

O que pudemos avaliar experimentalmente com a execução do programa é que apesar de ter 
encontrado bem as palavras-chave de cada texto (os termos considerados foram realmente 
relevantes), o algoritmo para clusterizar os textos não foi tão eficiente assim.

Aparentemente, textos com muitas palavras foram considerados muito mais que outros textos 
menores. Esta falha provavelmente se encontra no algoritmo do cálculo do potencial de 
semelhança entre os textos, descrito em alto-nível no algoritmo 2. Em uma próxima versão 
será interessante normalizar matemáticamente essa contagem para evitar esse tipo de 
discrepância de resultados.

Como não fiz medições do tempo de execução do programa para diversas entradas, a avaliação
da velocidade do programa para diferentes entradas será omitida.

\section{Conclusão}
\label{conclusao}

Neste trabalho construímos um sistema que permite separar por critérios de similaridade
(clusterizar) um conjunto de textos arbitrário através da identificação de 
tópicos/palavras-chave desse texto.

O trabalho conseguiu cumprir o propósito de praticar a linguagem C e o estudo de um 
problema mais complexo do que o de costume. A utilização massiva de estruturas de pesquisa
foi uma novidade. Uma decisão extremamente importante para o trabalho foi a de utilizarmos 
uma estrutura em forma de árvore binária ao invés de outras que poderíamos utilizar como,
por exemplo, tabelas hash o que provavelmente fez o programa executar com uma velocidade 
maior e gastando uma quantidade menor de espaço em memória.

A análise da variação da qualidade da similaridade avaliada pelo programa é uma avaliação 
subjetiva pois depende da leitura e interpretação dos textos retornados por parte de uma
pessoa (melhor avaliadora de similaridade de conteúdo possível). Porém, ao analisarmos a 
ocorrência de palavras chaves vemos que o sistema encontrou palavras que tinham realmente 
a ver com o conteúdo daquele texto e que não eram tão decorrentes assim nos outros (tinham 
certo caráter discriminativo entre os textos analisados).

Algumas melhorias que poderiam ser consideradas neste trabalho são:
\begin{itemize}
\item Um melhor tratamento dos textos de entrada (eliminando acentuação e outros 
caracteres inválidos dentro do programa);
\item Utilização de Stop Words, retirada de prefixo e sufixo e outros tipos de técnicas
aplicadas em recuperação de informação com o objetivo de maximizarmos o aproveitamento dos
termos contidos no índice;
\item Uma melhor análise de complexidade de espaço e tempo de execução do programa, talvez
a utilização de uma outra estrutura de dados para o índice seja mais eficiente em termos 
de execução;
\item Uma modelagem mais complexa da análise de frequência das ocorrências de termos 
relevantes nos documentos analisados;
\item Uma modelagem mais complexa do cálculo de similaridade entre os textos a partir de 
suas/seus palavras-chave/tópicos.
\end{itemize}

\end{document}
